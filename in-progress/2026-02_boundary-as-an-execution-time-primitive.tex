% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\section{Boundary as an Execution-Time Primitive for AI-Assisted
Software Development
Governance}\label{boundary-as-an-execution-time-primitive-for-ai-assisted-software-development-governance}

\subsection{Why AI-Assisted Development Fails Before Governance
Begins}\label{why-ai-assisted-development-fails-before-governance-begins}

\textbf{Author:} Spark Tsai\\
\textbf{ORCID:} https://orcid.org/0009-0006-8847-4703 \textbf{Email:}
spark.tsai@gmail.com \textbf{Date:} February 2026

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Abstract}\label{abstract}

Loss of control, behavioral drift, and non-auditability in AI-assisted
software development are commonly attributed to model misalignment,
hallucination, or insufficient guardrails. This paper argues that such
diagnoses overlook a fundamental category distinction.

We distinguish between \textbf{model alignment boundaries}, established
at training time through data distributions, RLHF, and safety
fine-tuning, and \textbf{task execution boundaries}, which must be
explicitly constructed at execution time for a specific engineering
task. While the former provides general, statistical safety tendencies,
it does not---and cannot---automatically inherit the concrete,
task-specific constraints required for engineering governance.

We show that many widely reported failures, including insecure yet
functional code generation, arise not from deficient model alignment but
from the absence of a \textbf{decidable task execution boundary} at
runtime. When such a boundary is missing, drift and violation become
epistemically undecidable, and model preferences fill the resulting
vacuum.

We formalize task execution boundaries as the resolution of visible
scope and explicit prohibitive constraints, introduce boundary evidence
as the minimal auditable unit, and demonstrate through engineering
scenarios that governance mechanisms operating without this primitive
rest on interpretive rather than decidable foundations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Introduction}\label{introduction}

AI-assisted software development has rapidly progressed from code
completion to autonomous agents capable of modifying production systems.
In response, governance efforts have largely focused on improving
\textbf{model alignment}: safer training data, stronger RLHF, and
increasingly sophisticated guardrails.

These efforts resemble attempts to build \textbf{\emph{safer engines}}.
Yet many engineering failures persist---not because the engine is
unsafe, but because the road has no speed limits.

This paper argues that a critical distinction has been overlooked
between two orthogonal categories of boundaries:

\begin{itemize}
\tightlist
\item
  \textbf{Model Alignment Boundaries (The ``Safer Engine''):} These are
  statistical, soft constraints learned during training. They ensure the
  ``car'' doesn't explode or intentionally crash. However, a safe engine
  alone doesn't know the difference between a highway and a
  sidewalk---it is inherently probabilistic and context-agnostic.
\item
  \textbf{Task Execution Boundaries (The ``Road Constraints''):} These
  are explicit, decidable, hard constraints constructed per task. Just
  as a driver needs to know the specific lane, direction, and speed
  limit of a particular street, an AI agent needs a defined action space
  (e.g., \emph{``MUST NOT modify files outside of \texttt{src/auth}''}).
\end{itemize}

We demonstrate that \textbf{model alignment boundaries do not---and
cannot---automatically inherit task-specific constraints.} Even the most
``aligned'' AI, like a high-performance car with the best brakes, can
still drive the wrong way down a one-way street if the \textbf{Task
Execution Boundary} is missing.

\subsubsection{From Narrative to Technical
Governance}\label{from-narrative-to-technical-governance}

Without an explicitly constructed boundary, governance collapses into
interpretation. When an AI agent drifts, the post-mortem becomes an
argument over ``intent'' rather than a verification of ``fact.''
Accountability becomes \textbf{narrative} (explaining what went wrong)
rather than \textbf{technical} (proving what was violated).

This paper proposes a framework to bridge this gap, transforming general
model safety into a decidable, evidence-based engineering governance
model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Related Work}\label{related-work}

\subsubsection{Behavioral Drift and Context
Degradation}\label{behavioral-drift-and-context-degradation}

Recent studies document various forms of behavioral degradation in
LLM-based agents, including semantic drift, goal drift, coordination
drift, and context degradation over extended interactions {[}1--5{]}.
These works provide valuable empirical observations and detection
techniques.

However, drift is typically treated as a detectable violation of an
assumed boundary. Few works examine the prior engineering condition:
whether a decidable boundary existed at execution time in the first
place.

\subsubsection{Guardrails and Policy
Enforcement}\label{guardrails-and-policy-enforcement}

Guardrail frameworks and policy-based enforcement mechanisms aim to
constrain undesirable outputs via filtering, rejection, or runtime
checks {[}6--9{]}. In AI-assisted coding, multiple studies report the
generation of insecure yet functional code, including hardcoded secrets
and weak cryptographic practices {[}10{]}.

These mechanisms operate post-generation and implicitly assume that a
permissible action space has already been defined.

\subsubsection{Agent Frameworks and Context
Management}\label{agent-frameworks-and-context-management}

Surveys of LLM-based agents for software engineering emphasize tool
orchestration, memory isolation, and context delivery {[}11--13{]}. The
Model Context Protocol (MCP) standardizes context transport and tool
access {[}14,15{]}.

While these frameworks improve infrastructure-level governance, they do
not define the decidable boundaries of the generative action space
itself.

\subsubsection{Model Alignment Boundary vs.~Task Execution
Boundary}\label{model-alignment-boundary-vs.-task-execution-boundary}

A substantial body of work addresses model alignment boundaries through
training-time techniques such as RLHF and safety fine-tuning. These
establish statistical, soft constraints applicable across tasks.

This literature often assumes that strong model alignment subsumes
task-specific execution boundaries. We argue that this is a category
error.

Model alignment boundaries and task execution boundaries differ in
origin, scope, and nature. They may have little or no intersection in
practice. Training-time safety does not imply execution-time governance.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{The Formalism of Task Execution
Boundaries}\label{the-formalism-of-task-execution-boundaries}

To transition from narrative governance to technical accountability, we
must define the primitives that constitute a boundary. We argue that a
boundary is not a static property of a model, but a dynamic resolution
of scope and constraints at execution time.

\subsubsection{Visible Scope}\label{visible-scope}

Visible scope refers to the set of artifacts accessible to the model at
execution time, such as specifications, tests, source code, explicit
goals, and context.\\
Scope defines \textbf{what the model can observe}, not what it is
permitted to do.\\
Scope alone does not bound behavior; it merely sets the possible action
space.

\subsubsection{Constraints as Prohibitive
Primitives}\label{constraints-as-prohibitive-primitives}

A critical distinction exists between prescriptive goals (MUST) and
prohibitive constraints (MUST NOT).

\begin{itemize}
\tightlist
\item
  \textbf{MUST} (Prescriptive): Expresses semantic completeness (e.g.,
  ``The code must be efficient''). Verifying ``completeness'' is
  computationally expensive and often involves subjective
  interpretation.
\item
  \textbf{MUST NOT} (Prohibitive): Expresses safety invariants (e.g.,
  ``The code must not hardcode secrets''). Prohibitions are
  \textbf{decidable}; they provide a clear, binary pass/fail condition
  that can be checked at runtime without semantic ambiguity.
\end{itemize}

In our framework, \textbf{decidability in governance emerges primarily
from prohibitive constraints.}

\subsubsection{Boundary as Execution-Time
Resolution}\label{boundary-as-execution-time-resolution}

The task execution boundary is the set of actions that the model is both
capable of generating (given what it can see) and explicitly permitted
to take (given what it is forbidden from doing) in a specific task
instance. In other words, it is the effective, permissible action space
at execution time---neither the full generative possibility nor a vague
safety guideline, but a concrete, decidable subset shaped by the
interplay of visibility and hard prohibitions.

We formalize the Task Execution Boundary as:

\[
B(t) = \{ a \mid a \in Possible(S_t) \land \forall c \in C, \neg c(a) \}
\]

\begin{itemize}
\tightlist
\item
  \(t\) --- Execution time (a specific task instance).
\item
  \(S_t\) --- Visible Scope at time \(t\): the set of artifacts
  accessible to the model during this execution (e.g., specific code
  files, API specifications, test suites, explicit goals, or context).
  Scope determines what the model \emph{can observe}, but not what it is
  \emph{permitted to do}.
\item
  \(C\) --- Prohibitive Constraints: the set of explicit MUST NOT
  predicates that define the invariants and ``no-go zones'' of the
  engineering task.\\
\item
  \(a\) --- An individual action or output generated by the model.
\item
  \(Possible(S_t)\) --- The set of all possible actions that the model
  could generate given the visible scope \(S_t\) (the unrestricted
  generative space).
\item
  \(B(t)\) or \(B_t\) --- The Task Execution Boundary: the resolved,
  permissible action space at time \(t\).
\end{itemize}

A critical corollary follows directly from the definition:

\[
\text{If } S_t = \emptyset \text{ or } C = \emptyset, \text{ then } B(t) = \bot
\]

When \(B(t) = \bot\) (boundary non-existence), the AI agent operates in
a \textbf{boundary vacuum}. In this state:

\begin{itemize}
\tightlist
\item
  No task-specific governance boundary constrains the generative action
  space.
\item
  Model-level statistical preferences (from training-time alignment)
  become the dominant determinant of output.
\item
  Drift, insecure code, or unintended behavior is not a deviation from a
  boundary---it is the default outcome of boundary absence.
\item
  Governance claims become epistemically undecidable: one cannot
  objectively determine whether an action is a violation, because no
  explicit permissible space was ever defined.
\end{itemize}

This explains why even highly ``aligned'' models frequently produce
insecure yet functional code (e.g., hardcoded secrets or weak
cryptography): they follow general probabilities because no
task-specific boundary was constructed to override them.

\subsubsection{Boundary Evidence}\label{boundary-evidence}

Boundary evidence records the resolved boundary state at execution time.
It represents the minimal auditable unit required for reproducibility
and governance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{execution\_record}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{id}\KeywordTok{:}\AttributeTok{ }\StringTok{"exec\_unique\_id"}
\AttributeTok{  }\FunctionTok{timestamp}\KeywordTok{:}\AttributeTok{ }\StringTok{"2026{-}02{-}06T14:30:00Z"}
\AttributeTok{  }\FunctionTok{scope}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{goals}\KeywordTok{:}\AttributeTok{ }\KeywordTok{[}\StringTok{"Replace auth logic with JWT"}\KeywordTok{]}
\AttributeTok{    }\FunctionTok{specifications}\KeywordTok{:}\AttributeTok{ }\KeywordTok{[}\StringTok{"docs/auth\_spec.md"}\KeywordTok{]}
\AttributeTok{    }\FunctionTok{tests}\KeywordTok{:}\AttributeTok{ }\KeywordTok{[}\StringTok{"tests/auth\_test.py"}\KeywordTok{]}
\AttributeTok{    }\FunctionTok{source\_code}\KeywordTok{:}\AttributeTok{ }\KeywordTok{[}\StringTok{"src/auth\_manager.py"}\KeywordTok{]}
\AttributeTok{  }\FunctionTok{constraints}\KeywordTok{:}
\AttributeTok{    }\KeywordTok{{-}}\AttributeTok{ }\StringTok{"MUST NOT modify files outside /src"}
\AttributeTok{    }\KeywordTok{{-}}\AttributeTok{ }\StringTok{"MUST NOT hardcode cryptographic secrets"}
\AttributeTok{    }\KeywordTok{{-}}\AttributeTok{ }\StringTok{"MUST NOT introduce new external dependencies"}
\AttributeTok{  }\FunctionTok{output\_hash}\KeywordTok{:}\AttributeTok{ }\StringTok{"sha256:..."}
\end{Highlighting}
\end{Shaded}

This representation is illustrative and does not prescribe a specific
storage format or governance system.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Model Boundary vs.~Task
Boundary}\label{model-boundary-vs.-task-boundary}

\subsubsection{Model Alignment Boundary (Training
Time)}\label{model-alignment-boundary-training-time}

\begin{itemize}
\tightlist
\item
  Established during training
\item
  Statistical, soft constraints
\item
  Cross-task generalization
\item
  Example: ``Avoid generating malware''
\end{itemize}

\subsubsection{Task Execution Boundary (Execution
Time)}\label{task-execution-boundary-execution-time}

\begin{itemize}
\tightlist
\item
  Constructed per execution
\item
  Explicit, hard constraints
\item
  Task-specific
\item
  Example: ``Do not modify tests/''
\end{itemize}

Model boundaries reduce the likelihood of harmful outputs. They do not
define permissible engineering actions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Engineering Scenarios}\label{engineering-scenarios}

We examine three scenarios involving modification of an existing login
system to use JWT authentication.

\subsubsection{7.1 Scenario A: Prompt
Only}\label{scenario-a-prompt-only}

\begin{itemize}
\tightlist
\item
  Model Alignment Boundary: Present
\item
  Task Execution Boundary: $\bot$
\end{itemize}

Outcome: Functional but insecure JWT implementation (e.g., hardcoded
secrets, weak algorithms). These outputs violate implicit engineering
expectations but not model alignment boundaries.

\subsubsection{7.2 Scenario B: Prompt +
SDD}\label{scenario-b-prompt-sdd}

\begin{itemize}
\tightlist
\item
  Model Alignment Boundary: Present
\item
  Task Execution Boundary: Partially formed
\end{itemize}

Outcome: Improved structural coherence but persistent insecure patterns.
Functional specifications do not constrain security decisions without
explicit prohibitions.

\subsubsection{7.3 Scenario C: Prompt + SDD + Explicit
Constraints}\label{scenario-c-prompt-sdd-explicit-constraints}

\begin{itemize}
\tightlist
\item
  Model Alignment Boundary: Present
\item
  Task Execution Boundary: Resolved
\end{itemize}

Explicit constraints such as ``MUST NOT hardcode secrets'' and ``MUST
NOT accept `none' algorithm'' confine generation to an auditable
subspace. Violations become decidable.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Discussion}\label{discussion}

\subsubsection{Drift as an Epistemic
Illusion}\label{drift-as-an-epistemic-illusion}

Behavioral drift is commonly treated as a stochastic property of model
behavior. We argue that in the absence of a task execution boundary,
drift is \textbf{epistemically undefined}. Without a formal boundary,
there is no reference baseline against which deviation can be measured.
In such cases, apparent drift reflects subjective human reinterpretation
of results rather than an objective system violation.

\subsubsection{Limitations and Future
Work}\label{limitations-and-future-work}

This paper focuses on the static resolution of boundaries at execution
time. We deliberately exclude dynamic boundary evolution, enforcement
mechanisms (e.g., runtime interception), and versioned lifecycle
governance. These concerns, while critical, presuppose the existence of
a decidable task execution boundary as defined here.

\subsubsection{Scope of Definition}\label{scope-of-definition}

The primitives defined in this work---visible scope and explicit
prohibitive constraints---represent the minimal set of conditions
required to construct a decidable boundary. While additional primitives
may emerge as AI agents evolve, we establish a necessary foundation:
without at least these elements, task-level governance remains
undecidable by design.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Conclusion}\label{conclusion}

This paper argues that many failures in AI-assisted software development
stem from a fundamental misdiagnosis. The problem is not merely
insufficient model alignment, but the \textbf{absence of a decidable
task execution boundary at runtime}.

Model alignment boundaries and task execution boundaries are orthogonal;
conflating them constitutes a \textbf{category error} that undermines
the validity of AI governance. By formalizing these boundaries and
introducing \textbf{boundary evidence} as the minimal auditable unit,
this work reframes AI governance as an execution-time engineering
problem rather than a training-time alignment deficiency.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{References}\label{references}

\begin{itemize}
\tightlist
\item
  {[}1{]} Abhishek Rath et al.~``Agent Drift: Quantifying Behavioral
  Degradation in Multi-Agent LLM Systems Over Extended Interactions.''
  arXiv:2601.04170, 2026.
\item
  {[}2{]} Lin Chen et al.~``AI Agent Behavioral Science.''
  arXiv:2506.06366v2, 2025.
\item
  {[}3{]} Technical Report: ``Evaluating Goal Drift in Language Model
  Agents.'' arXiv:2505.02709, 2025.
\item
  {[}4{]} Adnan Masood. ``Agent Drift: the reliability blind spot in
  multi-agent LLM systems.'' Medium, 2026.
\item
  {[}5{]} Various authors. Studies on context degradation and user
  behavior/data drift in LLMs (e.g., Deepchecks reports, 2024--2025).
\item
  {[}6{]} Y. Dong et al.~``Safeguarding Large Language Models: A
  Survey.'' arXiv:2406.02622, 2024. (Also published in Artificial
  Intelligence Review, 2025).
\item
  {[}7{]} ``Building Guardrails for Large Language Models.''
  arXiv:2402.01822, 2024.
\item
  {[}8{]} ``SoK: Evaluating Jailbreak Guardrails for Large Language
  Models.'' arXiv:2506.10597, 2025. {[}9{]} ``A Flexible Large Language
  Models Guardrail Development Methodology Applied to Off-Topic Prompt
  Detection.'' arXiv:2411.12946, 2024 (v2 2025).
\item
  {[}10{]} ``When Developer Aid Becomes Security Debt: A Systematic
  Analysis of Insecure Behaviors in LLM Coding Agents.''
  arXiv:2507.09329, 2025.
\item
  {[}11{]} Junwei Liu et al.~``Large Language Model-Based Agents for
  Software Engineering: A Survey.'' arXiv:2409.02977, 2024. (Accepted by
  TOSEM).
\item
  {[}12{]} ``From LLMs to LLM-based Agents for Software Engineering: A
  Survey of Current, Challenges and Future.'' arXiv:2408.02479v2.
\item
  {[}13{]} ``A Survey on Code Generation with LLM-based Agents.''
  arXiv:2508.00083, 2025.
\item
  {[}14{]} Anthropic. ``Introducing the Model Context Protocol (MCP).''
  Anthropic Announcements, November 2024.
  https://www.anthropic.com/news/model-context-protocol
\item
  {[}15{]} Model Context Protocol official documentation and subsequent
  engineering blogs (Anthropic, 2024--2025).
  https://modelcontextprotocol.io/
\end{itemize}

\end{document}
